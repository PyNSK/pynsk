<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyNSK - Новосибирское Python сообщество</title><link href="http://pynsk.ru/" rel="alternate"></link><link href="http://pynsk.ru/feeds/izvlechenie-informatsii.atom.xml" rel="self"></link><id>http://pynsk.ru/</id><updated>2015-09-23T10:00:00+06:00</updated><entry><title>Извлечение информации: grab - фреймворк для веб-парсинга</title><link href="http://pynsk.ru/posts/2015/Sep/23/izvlechenie-informatsii-grab-freimvork-dlia-veb-parsinga/" rel="alternate"></link><updated>2015-09-23T10:00:00+06:00</updated><author><name>Alexander Sapronov</name></author><id>tag:pynsk.ru,2015-09-23:posts/2015/Sep/23/izvlechenie-informatsii-grab-freimvork-dlia-veb-parsinga/</id><summary type="html">&lt;p&gt;Grab — python библиотека для парсинга сайтов&lt;/p&gt;
&lt;p&gt;Её основные функции:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Подготовка сетевого запроса (cookies, http-заголовки, POST/GET данные)&lt;/li&gt;
&lt;li&gt;Запрос на сервер (возможно через HTTP/SOCKS прокси)&lt;/li&gt;
&lt;li&gt;Получение ответа сервера и его первоначальная обработка (парсинг заголовков, парсинг cookies, определение кодировки документа, обработка редиректа (поддерживаются даже редирект в meta refresh тэге))&lt;/li&gt;
&lt;li&gt;Работа с DOM-деревом ответа (если это HTML-документ)&lt;/li&gt;
&lt;li&gt;Работа с формами (заполнение, автозаполнение)&lt;/li&gt;
&lt;li&gt;Отладка: логирование процесса в консоль, сетевых запросов и ответов в файлы&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В чем же особенность?&lt;/p&gt;
&lt;p&gt;Если scrapy — это реально паук, бегает по сети, тянет в тыщу потоков информацию, то grab — это скорее швейцарский нож, вы его берёте и начинает вдумчиво колупать сайт.&lt;/p&gt;
&lt;p&gt;Пощупать модуль:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://grablib.org/ru/"&gt;http://grablib.org/ru/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Image" src="https://s-media-cache-ak0.pinimg.com/736x/0e/ef/7e/0eef7e463459b10b3ea1f3f21b03d653.jpg" /&gt;&lt;/p&gt;</summary><category term="grab"></category><category term="parsing"></category><category term="web parsing"></category><category term="crawler"></category><category term="парсинг"></category></entry><entry><title>Извлечение информации: скачиваем список ссылок с помощью asyncio</title><link href="http://pynsk.ru/posts/2015/Sep/09/izvlechenie-informatsii-skachivaem-spisok-ssylok-s-pomoshchiu-asyncio/" rel="alternate"></link><updated>2015-09-09T10:00:00+06:00</updated><author><name>Alexander Sapronov</name></author><id>tag:pynsk.ru,2015-09-09:posts/2015/Sep/09/izvlechenie-informatsii-skachivaem-spisok-ssylok-s-pomoshchiu-asyncio/</id><summary type="html">&lt;p&gt;Порой возникают рутинные задачи, которые не хочется делать руками. Примером такой задачи может являться - скачать множество страниц по ссылкам. Если 5 ссылок еще вручную сохранить можно, а если их 1000? или 6250, как было в моем случае. &lt;/p&gt;
&lt;p&gt;На Python эту задачу можно с помощью модуля asyncio и aiohttp. &lt;/p&gt;
&lt;p&gt;Вот такой код можно написать за пару минут:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;asyncio&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;aiohttp&lt;/span&gt;

&lt;span class="nd"&gt;@asyncio.coroutine&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;yield from&lt;/span&gt; &lt;span class="n"&gt;aiohttp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;GET&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;yield from&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;yield from&lt;/span&gt; &lt;span class="n"&gt;save_page&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;# сохраняем страницу в файл, или еще куда&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;Exception&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

&lt;span class="nd"&gt;@asyncio.coroutine&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;download_parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;tasks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;asyncio&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;yield from&lt;/span&gt; &lt;span class="n"&gt;asyncio&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gather&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;tasks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;urls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="err"&gt;список&lt;/span&gt; &lt;span class="err"&gt;ссылок&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c"&gt;# список ссылок&lt;/span&gt;

&lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;asyncio&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_event_loop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;loop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run_until_complete&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;download_parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="asyncio"></category><category term="aiohttp"></category></entry><entry><title>Извлечение информации: парсинг дат/времени</title><link href="http://pynsk.ru/posts/2015/Aug/19/izvlechenie-informatsii-parsing-datvremeni/" rel="alternate"></link><updated>2015-08-19T08:00:00+06:00</updated><author><name>Alexander Sapronov</name></author><id>tag:pynsk.ru,2015-08-19:posts/2015/Aug/19/izvlechenie-informatsii-parsing-datvremeni/</id><summary type="html">&lt;p&gt;Извлечение информации из текста/web-страниц сопряжено с эвристиками. 
Одна из проблем, с которой сталкиваются разработчики - как парсить даты. Даты можно написать 100 и одним способом:
tomorrow, а еще 4/4/80 или March 5th, 1980. И какой код писать, чтобы понять эти даты?&lt;/p&gt;
&lt;p&gt;К счастью, код уже написан - &lt;a href="https://github.com/bear/parsedatetime"&gt;https://github.com/bear/parsedatetime&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Данная библиотека позволяет спарсить время в самых различных форматах.&lt;/p&gt;</summary><category term="парсинг"></category></entry><entry><title>Парсинг и Python: mechanize</title><link href="http://pynsk.ru/posts/2015/Aug/12/parsing-i-python-mechanize/" rel="alternate"></link><updated>2015-08-12T17:00:00+06:00</updated><author><name>Alexander Sapronov</name></author><id>tag:pynsk.ru,2015-08-12:posts/2015/Aug/12/parsing-i-python-mechanize/</id><summary type="html">&lt;p&gt;Для сбора данных с Web-страниц в Python существует библиотека mechanize, автоматизирующая взаимодействие с Web-сайтами. Часто используемая совместно с ней библиотека Beautiful Soup помогает понять тот «почти-HTML» код, который обычно находится на Web-сайтах.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.ibm.com/developerworks/ru/library/l-python-mechanize-beautiful-soup/"&gt;http://www.ibm.com/developerworks/ru/library/l-python-mechanize-beautiful-soup/&lt;/a&gt;&lt;/p&gt;</summary><category term="парсинг"></category><category term="mechanize"></category></entry><entry><title>Python и интересный парсинг сайтов</title><link href="http://pynsk.ru/posts/2015/Aug/05/python-i-interesnyi-parsing-saitov/" rel="alternate"></link><updated>2015-08-05T17:00:00+06:00</updated><author><name>Alexander Sapronov</name></author><id>tag:pynsk.ru,2015-08-05:posts/2015/Aug/05/python-i-interesnyi-parsing-saitov/</id><summary type="html">&lt;p&gt;Излечение информации со страниц может быть интересным. Рекомендуем к просмотру видео
&lt;a class='embedly-card' data-card-chrome='0' href='http://www.youtube.com/watch?v=hj-8l3AejNc'&gt;&lt;/a&gt;
            &lt;script&gt;
            !function(a){
                var b="embedly-platform",c="script";
                if(!a.getElementById(b)){
                    var d=a.createElement(c);
                    d.id=b;
                    d.src=("https:"===document.location.protocol?"https":"http")+"://cdn.embedly.com/widgets/platform.js";
                    var e=document.getElementsByTagName(c)[0];e.parentNode.insertBefore(d,e)}
                }(document);
            &lt;/script&gt;
            &lt;/p&gt;</summary><category term="видео"></category><category term="парсинг"></category></entry><entry><title>Парсинг с помощью asyncio</title><link href="http://pynsk.ru/posts/2015/Jul/29/parsing-s-pomoshchiu-asyncio/" rel="alternate"></link><updated>2015-07-29T17:00:00+06:00</updated><author><name>Alexander Sapronov</name></author><id>tag:pynsk.ru,2015-07-29:posts/2015/Jul/29/parsing-s-pomoshchiu-asyncio/</id><summary type="html">&lt;p&gt;Парсинг сайтов это вечная задача. Сайтов много, данных много и все они разные.
Для задач парсинга существует великое разнообразие инструментов.&lt;/p&gt;
&lt;p&gt;В Python 3-ей версии появился стандартный модуль для создания асинхронных программ. И грех не воспользоваться данной возможностью.&lt;/p&gt;
&lt;p&gt;Примером парсинга на asyncio может послужить эта статья 
&lt;a href="http://olegwock.net/blog/python/byistryiy-parsing-na-python-s-pomoshhyu-asyncio/"&gt;http://olegwock.net/blog/python/byistryiy-parsing-na-python-s-pomoshhyu-asyncio/&lt;/a&gt;&lt;/p&gt;</summary><category term="asyncio"></category><category term="парсинг"></category></entry></feed>